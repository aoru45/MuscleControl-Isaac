# Actor MLP configuration
layers:
  - units: 256
    activation: "relu"
    use_layer_norm: false
  - units: 256
    activation: "relu"
    use_layer_norm: false
